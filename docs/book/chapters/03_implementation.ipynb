{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Implementation Details\n",
    "\n",
    "This chapter explains how distfeat works internally, from data structures to algorithms. Understanding these details helps users extend the library and researchers reproduce the methods.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "distfeat follows a modular design with clear separation of concerns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main modules to show the structure\n",
    "import distfeat\n",
    "from distfeat import features, distances, normalization, io, config\n",
    "\n",
    "print(\"distfeat module structure:\")\n",
    "print(f\"  features:      {len([x for x in dir(features) if not x.startswith('_')])} public functions\")\n",
    "print(f\"  distances:     {len([x for x in dir(distances) if not x.startswith('_')])} public functions\")\n",
    "print(f\"  normalization: {len([x for x in dir(normalization) if not x.startswith('_')])} public functions\")\n",
    "print(f\"  io:            {len([x for x in dir(io) if not x.startswith('_')])} public functions\")\n",
    "print(f\"  config:        {len([x for x in dir(config) if not x.startswith('_')])} public functions\")\n",
    "\n",
    "print(\"\\nMain API functions:\")\n",
    "api_functions = [name for name in dir(distfeat) if not name.startswith('_')]\n",
    "for func in api_functions[:10]:  # Show first 10\n",
    "    print(f\"  - {func}\")\n",
    "if len(api_functions) > 10:\n",
    "    print(f\"  ... and {len(api_functions) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature System Implementation\n",
    "\n",
    "### Data Loading and Caching\n",
    "\n",
    "The feature system uses a global cache for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat.features import _initialize_features, get_feature_names, get_feature_system\n",
    "import time\n",
    "\n",
    "# Time the initialization\n",
    "start_time = time.time()\n",
    "_initialize_features()\n",
    "init_time = time.time() - start_time\n",
    "\n",
    "print(f\"Feature system initialization: {init_time:.3f} seconds\")\n",
    "\n",
    "# Show the cache structure\n",
    "feature_system = get_feature_system()\n",
    "feature_names = get_feature_names()\n",
    "\n",
    "print(f\"\\nCached data:\")\n",
    "print(f\"  Phonemes: {len(feature_system)}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "print(f\"  Memory usage: ~{len(feature_system) * len(feature_names) * 4 / 1024:.1f} KB\")\n",
    "\n",
    "# Show feature names\n",
    "print(f\"\\nFeature types:\")\n",
    "feature_categories = {\n",
    "    'Major class': ['consonantal', 'sonorant', 'syllabic'],\n",
    "    'Place': ['labial', 'coronal', 'dorsal', 'pharyngeal'],\n",
    "    'Manner': ['continuant', 'nasal', 'lateral', 'strident'],\n",
    "    'Voicing': ['voice', 'spread', 'constricted']\n",
    "}\n",
    "\n",
    "for category, examples in feature_categories.items():\n",
    "    available = [f for f in examples if f in feature_names]\n",
    "    print(f\"  {category}: {available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Vector Representation\n",
    "\n",
    "Phonemes are represented as binary feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import phoneme_to_features\n",
    "import numpy as np\n",
    "\n",
    "# Examine feature vectors for different sound types\n",
    "sound_examples = {\n",
    "    'Voiceless stop': 'p',\n",
    "    'Voiced stop': 'b', \n",
    "    'Fricative': 'f',\n",
    "    'Nasal': 'm',\n",
    "    'Vowel': 'a'\n",
    "}\n",
    "\n",
    "print(\"Feature vector examples:\")\n",
    "print(\"(Showing first 10 features only)\\n\")\n",
    "\n",
    "# Get feature names for display\n",
    "feature_names = get_feature_names()\n",
    "display_features = feature_names[:10]\n",
    "\n",
    "# Header\n",
    "print(f\"{'Sound':15}\", end=\"\")\n",
    "for feat in display_features:\n",
    "    print(f\"{feat[:8]:>9}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (15 + 9 * len(display_features)))\n",
    "\n",
    "# Feature vectors\n",
    "for sound_type, phoneme in sound_examples.items():\n",
    "    features = phoneme_to_features(phoneme)\n",
    "    if features:\n",
    "        print(f\"{sound_type} [{phoneme}]:    \", end=\"\")\n",
    "        for feat in display_features:\n",
    "            value = features.get(feat, 0)\n",
    "            print(f\"{value:9}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "print(f\"\\nTotal features per phoneme: {len(feature_names)}\")\n",
    "print(f\"Storage: {len(feature_names)} bits = {len(feature_names) // 8} bytes per phoneme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations\n",
    "\n",
    "Several optimizations improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import calculate_distance\n",
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "# Demonstrate caching performance\n",
    "phoneme_pairs = [('p', 'b'), ('t', 'd'), ('k', 'g')] * 100\n",
    "\n",
    "# Time with cache\n",
    "start_time = time.time()\n",
    "for p1, p2 in phoneme_pairs:\n",
    "    calculate_distance(p1, p2)\n",
    "cached_time = time.time() - start_time\n",
    "\n",
    "print(f\"Cached distance calculations:\")\n",
    "print(f\"  300 distance calls: {cached_time:.3f} seconds\")\n",
    "print(f\"  ~{300/cached_time:.0f} calls per second\")\n",
    "\n",
    "# Show cache info (internal)\n",
    "print(f\"\\nCaching benefits:\")\n",
    "print(f\"  - Feature vectors cached globally\")\n",
    "print(f\"  - Distance calculations use LRU cache\")\n",
    "print(f\"  - NumPy vectorization for matrix operations\")\n",
    "print(f\"  - Lazy initialization of feature system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Calculation Implementation\n",
    "\n",
    "### Core Distance Functions\n",
    "\n",
    "Each distance metric has a specific implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat.distances import (\n",
    "    _hamming_distance, _jaccard_distance, _euclidean_distance,\n",
    "    _cosine_distance, _manhattan_distance\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Create example feature vectors\n",
    "vec1 = np.array([1, 0, 1, 0, 1])  # Pattern: 10101\n",
    "vec2 = np.array([1, 1, 0, 0, 1])  # Pattern: 11001\n",
    "\n",
    "print(\"Distance implementations:\")\n",
    "print(f\"Vector 1: {vec1}\")\n",
    "print(f\"Vector 2: {vec2}\")\n",
    "print()\n",
    "\n",
    "# Calculate each distance\n",
    "hamming = _hamming_distance(vec1, vec2, normalize=True)\n",
    "jaccard = _jaccard_distance(vec1, vec2)\n",
    "euclidean = _euclidean_distance(vec1, vec2, normalize=True)\n",
    "cosine = _cosine_distance(vec1, vec2)\n",
    "manhattan = _manhattan_distance(vec1, vec2, normalize=True)\n",
    "\n",
    "print(f\"Hamming distance:   {hamming:.3f} (differs in {np.sum(vec1 != vec2)}/5 positions)\")\n",
    "print(f\"Jaccard distance:   {jaccard:.3f} (1 - intersection/union)\")\n",
    "print(f\"Euclidean distance: {euclidean:.3f} (L2 norm, normalized)\")\n",
    "print(f\"Cosine distance:    {cosine:.3f} (1 - cosine similarity)\")\n",
    "print(f\"Manhattan distance: {manhattan:.3f} (L1 norm, normalized)\")\n",
    "\n",
    "print(\"\\nImplementation details:\")\n",
    "print(\"  - All distances normalized to [0,1] range\")\n",
    "print(\"  - Hamming: simple XOR operation\")\n",
    "print(\"  - Jaccard: set intersection/union on active features\")\n",
    "print(\"  - Euclidean/Manhattan: standard Lp norms\")\n",
    "print(\"  - Cosine: handles zero vectors gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering Distance\n",
    "\n",
    "The K-means method is more complex, involving clustering and centroid distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import build_distance_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Demonstrate K-means clustering process\n",
    "phonemes = ['p', 'b', 't', 'd', 'k', 'g', 'm', 'n', 'ŋ', 'f', 's', 'x']\n",
    "\n",
    "print(\"K-means distance calculation process:\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "# Step 1: Get feature vectors\n",
    "feature_vectors = []\n",
    "for p in phonemes:\n",
    "    features = phoneme_to_features(p)\n",
    "    if features:\n",
    "        vec = [features.get(f, 0) for f in feature_names]\n",
    "        feature_vectors.append(vec)\n",
    "\n",
    "X = np.array(feature_vectors)\n",
    "print(f\"Step 1: Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Step 2: K-means clustering  \n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print(f\"Step 2: K-means with {n_clusters} clusters\")\n",
    "print(f\"        Inertia: {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Show cluster assignments\n",
    "print(\"\\nStep 3: Cluster assignments:\")\n",
    "for i, (phoneme, cluster_id) in enumerate(zip(phonemes, clusters)):\n",
    "    print(f\"  [{phoneme}] → Cluster {cluster_id}\")\n",
    "\n",
    "# Step 4: Calculate centroid distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "centroid_distances = pairwise_distances(centroids)\n",
    "max_dist = np.max(centroid_distances)\n",
    "normalized_distances = centroid_distances / max_dist if max_dist > 0 else centroid_distances\n",
    "\n",
    "print(f\"\\nStep 4: Centroid distance matrix:\")\n",
    "print(\"   \", end=\"\")\n",
    "for i in range(n_clusters):\n",
    "    print(f\"C{i:2}\", end=\"\")\n",
    "print()\n",
    "for i in range(n_clusters):\n",
    "    print(f\"C{i} \", end=\"\")\n",
    "    for j in range(n_clusters):\n",
    "        print(f\"{normalized_distances[i,j]:5.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Final result\n",
    "print(f\"\\nStep 5: Final phoneme distances based on cluster membership\")\n",
    "print(f\"        Same cluster → distance 0\")\n",
    "print(f\"        Different clusters → distance from centroid matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Matrix Construction\n",
    "\n",
    "Building full distance matrices efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Time matrix construction\n",
    "test_phonemes = ['p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 's', 'z']\n",
    "n = len(test_phonemes)\n",
    "\n",
    "print(f\"Distance matrix construction for {n} phonemes:\")\n",
    "print(f\"Matrix size: {n}×{n} = {n*n} cells\")\n",
    "print(f\"Unique distances: {n*(n-1)//2} (due to symmetry)\")\n",
    "\n",
    "# Time different methods\n",
    "methods = ['hamming', 'jaccard', 'euclidean', 'kmeans']\n",
    "for method in methods:\n",
    "    start_time = time.time()\n",
    "    matrix, labels = build_distance_matrix(test_phonemes, method=method)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    unique_values = len(np.unique(matrix))\n",
    "    print(f\"  {method:10}: {elapsed:.3f}s, {unique_values:2d} unique distances\")\n",
    "\n",
    "print(\"\\nOptimizations:\")\n",
    "print(\"  - Symmetric matrices: compute upper triangle only\")\n",
    "print(\"  - Vectorized operations where possible\")\n",
    "print(\"  - Caching of feature lookups\")\n",
    "print(\"  - K-means: single clustering, reuse centroids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Implementation\n",
    "\n",
    "### Unicode Normalization Pipeline\n",
    "\n",
    "IPA normalization involves several steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat.normalization import normalize_ipa, normalize_glyph\n",
    "import unicodedata\n",
    "\n",
    "# Demonstrate normalization steps\n",
    "test_cases = [\n",
    "    \"pʰæ̃n\",        # Aspirated, nasalized\n",
    "    \"t̪ʰin\",        # Dental, aspirated\n",
    "    \"kʷā\",         # Labialized, long\n",
    "    \"ʃi:\",          # ASCII length mark\n",
    "    \"tɕʰi\",        # Complex consonant + aspiration\n",
    "]\n",
    "\n",
    "print(\"IPA Normalization Pipeline:\")\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "for original in test_cases:\n",
    "    print(f\"Original:  '{original}'\")\n",
    "    print(f\"  Length: {len(original)} chars\")\n",
    "    \n",
    "    # Show Unicode decomposition\n",
    "    nfd = unicodedata.normalize('NFD', original)\n",
    "    print(f\"  NFD:    '{nfd}' ({len(nfd)} chars)\")\n",
    "    \n",
    "    # Full normalization\n",
    "    normalized = normalize_ipa(original)\n",
    "    print(f\"  Final:  '{normalized}' ({len(normalized)} chars)\")\n",
    "    \n",
    "    # Character breakdown\n",
    "    chars = []\n",
    "    for char in normalized:\n",
    "        name = unicodedata.name(char, f'U+{ord(char):04X}')\n",
    "        chars.append(f'{char}({name[:20]}...)')\n",
    "    print(f\"  Chars:  {' + '.join(chars)}\")\n",
    "    print()\n",
    "\n",
    "print(\"Normalization steps:\")\n",
    "print(\"  1. NFD decomposition (separate base + diacritics)\")\n",
    "print(\"  2. Diacritic reordering (consistent order)\")\n",
    "print(\"  3. ASCII substitutions (: → ː)\")\n",
    "print(\"  4. Case normalization (optional)\")\n",
    "print(\"  5. Tone handling (preserve or remove)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O and Serialization\n",
    "\n",
    "### Matrix Export Formats\n",
    "\n",
    "distfeat supports multiple export formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import save_distance_matrix, load_distance_matrix\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create a small test matrix\n",
    "test_phonemes = ['p', 'b', 't']\n",
    "matrix, labels = build_distance_matrix(test_phonemes)\n",
    "\n",
    "print(\"Export format examples:\")\n",
    "print(\"======================\\n\")\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # TSV format\n",
    "    tsv_path = os.path.join(tmpdir, 'test.tsv')\n",
    "    save_distance_matrix(matrix, labels, tsv_path, format='tsv')\n",
    "    \n",
    "    with open(tsv_path, 'r') as f:\n",
    "        tsv_content = f.read()\n",
    "    print(\"TSV format (UNIPA compatible):\")\n",
    "    print(tsv_content)\n",
    "    \n",
    "    # JSON format\n",
    "    json_path = os.path.join(tmpdir, 'test.json')\n",
    "    save_distance_matrix(matrix, labels, json_path, format='json')\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    print(\"JSON format (with metadata):\")\n",
    "    print(f\"  phonemes: {json_data['phonemes']}\")\n",
    "    print(f\"  matrix: {len(json_data['matrix'])}x{len(json_data['matrix'][0])}\")\n",
    "    print(f\"  metadata: {json_data['metadata']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test round-trip\n",
    "    loaded_matrix, loaded_labels = load_distance_matrix(tsv_path)\n",
    "    \n",
    "    print(\"Round-trip test:\")\n",
    "    print(f\"  Original shape: {matrix.shape}\")\n",
    "    print(f\"  Loaded shape:   {loaded_matrix.shape}\")\n",
    "    print(f\"  Max difference: {np.max(np.abs(matrix - loaded_matrix)):.6f}\")\n",
    "    print(f\"  Labels match:   {labels == loaded_labels}\")\n",
    "\n",
    "print(\"\\nSupported formats:\")\n",
    "print(\"  - TSV: tab-separated, compatible with UNIPA\")\n",
    "print(\"  - CSV: comma-separated, Excel-compatible\")\n",
    "print(\"  - JSON: with metadata, web-friendly\")\n",
    "print(\"  - NumPy: binary format for Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Robustness\n",
    "\n",
    "### Graceful Degradation\n",
    "\n",
    "distfeat handles various error conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import phoneme_to_features, calculate_distance\n",
    "import logging\n",
    "\n",
    "# Configure logging to see warnings\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "print(\"Error handling examples:\")\n",
    "print(\"========================\\n\")\n",
    "\n",
    "# Test missing phoneme handling\n",
    "print(\"1. Unknown phonemes:\")\n",
    "unknown_phonemes = ['zzz', '☃', 'invalid']\n",
    "for phoneme in unknown_phonemes:\n",
    "    # Default behavior: warn and return None\n",
    "    result = phoneme_to_features(phoneme, on_error='warn')\n",
    "    print(f\"   '{phoneme}' → {result}\")\n",
    "\n",
    "print(\"\\n2. Distance calculation with missing phonemes:\")\n",
    "valid_distance = calculate_distance('p', 'b')\n",
    "invalid_distance = calculate_distance('p', 'zzz')\n",
    "print(f\"   p-b distance: {valid_distance}\")\n",
    "print(f\"   p-zzz distance: {invalid_distance}\")\n",
    "\n",
    "print(\"\\n3. Matrix operations with mixed valid/invalid:\")\n",
    "mixed_phonemes = ['p', 'b', 'invalid', 't']\n",
    "try:\n",
    "    matrix, labels = build_distance_matrix(mixed_phonemes)\n",
    "    print(f\"   Matrix shape: {matrix.shape}\")\n",
    "    print(f\"   Non-zero entries: {np.count_nonzero(matrix)}\")\n",
    "    print(f\"   Max distance: {np.max(matrix):.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print(\"\\nError handling modes:\")\n",
    "print(\"  - 'warn': Log warning, return None (default)\")\n",
    "print(\"  - 'ignore': Silent failure, return None\")\n",
    "print(\"  - 'raise': Raise ValueError exception\")\n",
    "print(\"  - Invalid phonemes get max distance (1.0)\")\n",
    "print(\"  - Matrices handle partial data gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "### Scalability Analysis\n",
    "\n",
    "Understanding performance characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test scaling behavior\n",
    "print(\"Performance scaling analysis:\")\n",
    "print(\"============================\\n\")\n",
    "\n",
    "# Get available phonemes\n",
    "feature_system = get_feature_system()\n",
    "all_phonemes = list(feature_system.keys())\n",
    "\n",
    "# Test different matrix sizes\n",
    "sizes = [10, 20, 50, 100, 200]\n",
    "times = []\n",
    "\n",
    "print(\"Matrix size vs construction time:\")\n",
    "for size in sizes:\n",
    "    if size <= len(all_phonemes):\n",
    "        phonemes = all_phonemes[:size]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        matrix, _ = build_distance_matrix(phonemes, method='hamming')\n",
    "        elapsed = time.time() - start_time\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        operations = size * (size - 1) // 2\n",
    "        ops_per_sec = operations / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        print(f\"  {size:3d} phonemes: {elapsed:.3f}s ({ops_per_sec:.0f} ops/sec)\")\n",
    "    else:\n",
    "        times.append(None)\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\nMemory usage estimates:\")\n",
    "feature_count = len(get_feature_names())\n",
    "phoneme_count = len(all_phonemes)\n",
    "\n",
    "feature_cache_mb = (phoneme_count * feature_count * 4) / (1024 * 1024)\n",
    "matrix_1000_mb = (1000 * 1000 * 8) / (1024 * 1024)\n",
    "\n",
    "print(f\"  Feature cache: {feature_cache_mb:.1f} MB ({phoneme_count} phonemes × {feature_count} features)\")\n",
    "print(f\"  1000×1000 matrix: {matrix_1000_mb:.1f} MB (float64)\")\n",
    "\n",
    "# Complexity analysis\n",
    "print(f\"\\nComplexity analysis:\")\n",
    "print(f\"  Single distance: O(F) where F = feature count\")\n",
    "print(f\"  Distance matrix: O(N²·F) where N = phoneme count\")\n",
    "print(f\"  K-means matrix: O(K·N·F·I) where K = clusters, I = iterations\")\n",
    "print(f\"  Memory: O(N·F + N²) for cache + matrix\")\n",
    "\n",
    "print(f\"\\nOptimization strategies:\")\n",
    "print(f\"  - Use symmetric matrices (store upper triangle only)\")\n",
    "print(f\"  - Cache frequently accessed distances\")\n",
    "print(f\"  - Batch process multiple phoneme pairs\")\n",
    "print(f\"  - Consider sparse matrices for large, sparse datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Points\n",
    "\n",
    "### Adding Custom Distance Methods\n",
    "\n",
    "Users can register custom distance functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import register_distance_method, calculate_distance\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom weighted Hamming distance\n",
    "def weighted_hamming(vec1, vec2):\n",
    "    \"\"\"Hamming distance with feature importance weighting.\"\"\"\n",
    "    # Example weights (in practice, learn from data)\n",
    "    weights = np.linspace(0.5, 2.0, len(vec1))  # Later features weighted more\n",
    "    differences = (vec1 != vec2).astype(float)\n",
    "    weighted_diff = np.sum(weights * differences)\n",
    "    return weighted_diff / np.sum(weights)  # Normalize\n",
    "\n",
    "# Register the custom method\n",
    "register_distance_method('weighted_hamming', weighted_hamming)\n",
    "\n",
    "print(\"Custom distance method example:\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "# Test the custom method\n",
    "pairs = [('p', 'b'), ('p', 't'), ('p', 'a')]\n",
    "for p1, p2 in pairs:\n",
    "    standard = calculate_distance(p1, p2, method='hamming')\n",
    "    custom = calculate_distance(p1, p2, method='weighted_hamming')\n",
    "    print(f\"  {p1}-{p2}: standard={standard:.3f}, weighted={custom:.3f}\")\n",
    "\n",
    "print(\"\\nCustom method requirements:\")\n",
    "print(\"  - Function signature: (vec1, vec2) -> float\")\n",
    "print(\"  - Input: numpy arrays of same length\")\n",
    "print(\"  - Output: non-negative distance value\")\n",
    "print(\"  - Register with register_distance_method()\")\n",
    "print(\"  - Works with all distfeat functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Feature Systems\n",
    "\n",
    "Loading and using custom feature systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a minimal custom feature system\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "print(\"Custom feature system example:\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "# Create sample data\n",
    "custom_data = [\n",
    "    ['phoneme', 'vowel', 'high', 'back', 'round'],\n",
    "    ['i', '1', '1', '0', '0'],\n",
    "    ['u', '1', '1', '1', '1'], \n",
    "    ['a', '1', '0', '0', '0'],\n",
    "    ['p', '0', '0', '0', '0'],\n",
    "    ['k', '0', '0', '1', '0']\n",
    "]\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(custom_data)\n",
    "    custom_path = f.name\n",
    "\n",
    "try:\n",
    "    # Load custom system\n",
    "    from distfeat import load_custom_features\n",
    "    load_custom_features(\n",
    "        custom_path,\n",
    "        name='minimal',\n",
    "        delimiter=',',\n",
    "        phoneme_col='phoneme'\n",
    "    )\n",
    "    \n",
    "    print(\"Custom system loaded successfully!\")\n",
    "    \n",
    "    # Use custom system\n",
    "    custom_features = phoneme_to_features('i', system='minimal')\n",
    "    print(f\"Features for 'i' in custom system: {custom_features}\")\n",
    "    \n",
    "    # Calculate distance with custom system\n",
    "    # Note: This would need system parameter support in calculate_distance\n",
    "    print(\"\\nCustom systems support:\")\n",
    "    print(\"  - CSV/TSV files with any delimiter\")\n",
    "    print(\"  - Flexible column naming\")\n",
    "    print(\"  - Binary or numeric features\")\n",
    "    print(\"  - Multiple systems simultaneously\")\n",
    "    \n",
    "finally:\n",
    "    os.unlink(custom_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "distfeat's implementation emphasizes:\n",
    "\n",
    "1. **Performance**: Caching, vectorization, and optimized data structures\n",
    "2. **Robustness**: Graceful error handling and input validation\n",
    "3. **Extensibility**: Plugin architecture for custom methods and features\n",
    "4. **Compatibility**: Multiple I/O formats and Unicode normalization\n",
    "5. **Scalability**: Efficient algorithms for large phoneme inventories\n",
    "\n",
    "The modular design allows users to leverage individual components while the unified API provides convenience for common tasks.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Validation**: See how distfeat performs on real data in [Chapter 4](04_validation.ipynb)\n",
    "- **Applications**: Explore use cases in the [Case Studies](../case_studies/indo_european.ipynb)\n",
    "- **API Reference**: Detailed function documentation in [API docs](../api/features.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}