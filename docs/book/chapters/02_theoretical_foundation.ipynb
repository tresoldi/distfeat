{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Theoretical Foundation\n",
    "\n",
    "## Phonetic Features: From Articulation to Computation\n",
    "\n",
    "### The Articulatory Basis\n",
    "\n",
    "Human speech sounds are produced by coordinated movements of the vocal tract. These movements can be decomposed into independent components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from distfeat import phoneme_to_features\n",
    "\n",
    "# Analyze stops by place and voicing\n",
    "stops_data = []\n",
    "for place in [('p', 'b', 'Bilabial'), ('t', 'd', 'Alveolar'), ('k', 'g', 'Velar')]:\n",
    "    voiceless, voiced, place_name = place\n",
    "    \n",
    "    vl_features = phoneme_to_features(voiceless)\n",
    "    vd_features = phoneme_to_features(voiced)\n",
    "    \n",
    "    stops_data.append({\n",
    "        'Phoneme': voiceless,\n",
    "        'Place': place_name,\n",
    "        'Voice': vl_features.get('voice', 0),\n",
    "        'Labial': vl_features.get('labial', 0),\n",
    "        'Coronal': vl_features.get('coronal', 0),\n",
    "        'Dorsal': vl_features.get('dorsal', 0),\n",
    "        'Continuant': vl_features.get('continuant', 0)\n",
    "    })\n",
    "    \n",
    "    stops_data.append({\n",
    "        'Phoneme': voiced,\n",
    "        'Place': place_name,\n",
    "        'Voice': vd_features.get('voice', 0),\n",
    "        'Labial': vd_features.get('labial', 0),\n",
    "        'Coronal': vd_features.get('coronal', 0),\n",
    "        'Dorsal': vd_features.get('dorsal', 0),\n",
    "        'Continuant': vd_features.get('continuant', 0)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(stops_data)\n",
    "print(\"Feature Matrix for Stops:\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"1. Voice is the only difference within each place\")\n",
    "print(\"2. Place features (labial/coronal/dorsal) distinguish columns\")\n",
    "print(\"3. All stops share continuant=0 (complete closure)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Geometry and Natural Classes\n",
    "\n",
    "Features are not independent but organized hierarchically. This organization predicts which sounds pattern together in phonological processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import get_feature_system\n",
    "import numpy as np\n",
    "\n",
    "# Find natural classes\n",
    "def find_natural_class(phonemes, feature_system):\n",
    "    \"\"\"Find features that define a natural class.\"\"\"\n",
    "    if not phonemes:\n",
    "        return {}\n",
    "    \n",
    "    # Get features for all phonemes\n",
    "    feature_sets = []\n",
    "    for p in phonemes:\n",
    "        if p in feature_system:\n",
    "            feature_sets.append(feature_system[p]['features'])\n",
    "    \n",
    "    if not feature_sets:\n",
    "        return {}\n",
    "    \n",
    "    # Find common features\n",
    "    common = {}\n",
    "    for feature in feature_sets[0]:\n",
    "        values = [fs.get(feature) for fs in feature_sets]\n",
    "        if all(v == values[0] for v in values):\n",
    "            common[feature] = values[0]\n",
    "    \n",
    "    return common\n",
    "\n",
    "# Test with natural classes\n",
    "feature_system = get_feature_system()\n",
    "\n",
    "natural_classes = {\n",
    "    'Voiceless stops': ['p', 't', 'k'],\n",
    "    'Voiced stops': ['b', 'd', 'g'],\n",
    "    'Nasals': ['m', 'n', 'ŋ'],\n",
    "    'Fricatives': ['f', 's', 'ʃ', 'x'],\n",
    "    'High vowels': ['i', 'u'],\n",
    "    'Low vowels': ['a', 'ɑ']\n",
    "}\n",
    "\n",
    "for class_name, phonemes in natural_classes.items():\n",
    "    common = find_natural_class(phonemes, feature_system)\n",
    "    defining_features = {k: v for k, v in common.items() \n",
    "                        if k in ['voice', 'nasal', 'continuant', 'high', 'low', 'consonantal']}\n",
    "    print(f\"\\n{class_name}: {phonemes}\")\n",
    "    print(f\"  Defining features: {defining_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics: From Features to Similarity\n",
    "\n",
    "### Hamming Distance\n",
    "\n",
    "The simplest metric counts feature differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import calculate_distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Demonstrate Hamming distance calculation\n",
    "def show_hamming_calculation(p1, p2):\n",
    "    f1 = phoneme_to_features(p1)\n",
    "    f2 = phoneme_to_features(p2)\n",
    "    \n",
    "    differences = 0\n",
    "    diff_features = []\n",
    "    \n",
    "    for feature in f1:\n",
    "        if f1[feature] != f2.get(feature, 0):\n",
    "            differences += 1\n",
    "            diff_features.append(feature)\n",
    "    \n",
    "    total = len(f1)\n",
    "    normalized = differences / total\n",
    "    \n",
    "    print(f\"Hamming distance between [{p1}] and [{p2}]:\")\n",
    "    print(f\"  Different features: {differences}/{total}\")\n",
    "    print(f\"  Normalized distance: {normalized:.3f}\")\n",
    "    print(f\"  Differing in: {', '.join(diff_features[:5])}...\")\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Examples\n",
    "pairs = [('p', 'b'), ('p', 'f'), ('p', 't'), ('p', 'a')]\n",
    "distances = []\n",
    "\n",
    "for p1, p2 in pairs:\n",
    "    dist = show_hamming_calculation(p1, p2)\n",
    "    distances.append(dist)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Distance\n",
    "\n",
    "Jaccard distance considers only active features, making it less sensitive to the coding scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Hamming vs Jaccard\n",
    "def compare_metrics(p1, p2):\n",
    "    f1 = phoneme_to_features(p1)\n",
    "    f2 = phoneme_to_features(p2)\n",
    "    \n",
    "    # Calculate Jaccard components\n",
    "    active1 = {k for k, v in f1.items() if v == 1}\n",
    "    active2 = {k for k, v in f2.items() if v == 1}\n",
    "    \n",
    "    intersection = active1 & active2\n",
    "    union = active1 | active2\n",
    "    \n",
    "    jaccard_sim = len(intersection) / len(union) if union else 0\n",
    "    jaccard_dist = 1 - jaccard_sim\n",
    "    \n",
    "    hamming = calculate_distance(p1, p2, method='hamming')\n",
    "    \n",
    "    print(f\"[{p1}] vs [{p2}]:\")\n",
    "    print(f\"  Active features in [{p1}]: {len(active1)}\")\n",
    "    print(f\"  Active features in [{p2}]: {len(active2)}\")\n",
    "    print(f\"  Shared active features: {len(intersection)}\")\n",
    "    print(f\"  Jaccard distance: {jaccard_dist:.3f}\")\n",
    "    print(f\"  Hamming distance: {hamming:.3f}\")\n",
    "    print(f\"  Difference: {abs(jaccard_dist - hamming):.3f}\")\n",
    "\n",
    "# Compare consonant-vowel pairs\n",
    "compare_metrics('p', 'a')\n",
    "print()\n",
    "compare_metrics('t', 'i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-Based Distance\n",
    "\n",
    "K-means clustering groups similar sounds, creating a coarser but more robust distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import build_distance_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compare regular distance with k-means\n",
    "phonemes = ['p', 'b', 't', 'd', 'k', 'g', 'm', 'n', 'f', 's']\n",
    "\n",
    "# Regular Hamming distance\n",
    "hamming_matrix, _ = build_distance_matrix(phonemes, method='hamming')\n",
    "\n",
    "# K-means distance (with 5 clusters)\n",
    "kmeans_matrix, _ = build_distance_matrix(phonemes, method='kmeans', n_clusters=5)\n",
    "\n",
    "# Visualize both\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(hamming_matrix, xticklabels=phonemes, yticklabels=phonemes,\n",
    "            annot=True, fmt='.2f', cmap='YlOrRd', ax=ax1, vmin=0, vmax=1)\n",
    "ax1.set_title('Hamming Distance')\n",
    "\n",
    "sns.heatmap(kmeans_matrix, xticklabels=phonemes, yticklabels=phonemes,\n",
    "            annot=True, fmt='.2f', cmap='YlOrRd', ax=ax2, vmin=0, vmax=1)\n",
    "ax2.set_title('K-means Distance (5 clusters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze cluster structure\n",
    "unique_distances = np.unique(kmeans_matrix)\n",
    "print(f\"\\nK-means creates {len(unique_distances)} unique distance values\")\n",
    "print(f\"Distance values: {unique_distances}\")\n",
    "print(\"\\nThis discretization can improve robustness for cognate detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory and Feature Importance\n",
    "\n",
    "Not all features contribute equally to phonetic distance. We can quantify feature importance using information theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import get_feature_system\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Calculate feature entropy\n",
    "feature_system = get_feature_system()\n",
    "feature_names = list(next(iter(feature_system.values()))['features'].keys())\n",
    "\n",
    "# Count feature frequencies\n",
    "feature_counts = {f: {'0': 0, '1': 0} for f in feature_names[:20]}  # First 20 features\n",
    "\n",
    "for phoneme_data in feature_system.values():\n",
    "    features = phoneme_data['features']\n",
    "    for f in feature_counts:\n",
    "        value = str(features.get(f, 0))\n",
    "        feature_counts[f][value] += 1\n",
    "\n",
    "# Calculate entropy for each feature\n",
    "feature_entropies = {}\n",
    "for feature, counts in feature_counts.items():\n",
    "    total = counts['0'] + counts['1']\n",
    "    if total > 0:\n",
    "        probs = [counts['0']/total, counts['1']/total]\n",
    "        feature_entropies[feature] = entropy(probs, base=2)\n",
    "\n",
    "# Sort by entropy\n",
    "sorted_features = sorted(feature_entropies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Information Content (bits):\")\n",
    "print(\"\\nHigh entropy (most informative):\")\n",
    "for feature, ent in sorted_features[:5]:\n",
    "    print(f\"  {feature:20} {ent:.3f} bits\")\n",
    "\n",
    "print(\"\\nLow entropy (least informative):\")\n",
    "for feature, ent in sorted_features[-5:]:\n",
    "    print(f\"  {feature:20} {ent:.3f} bits\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- High entropy features distinguish many phoneme pairs\")\n",
    "print(\"- Low entropy features are redundant or rarely used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality Theory and Distance\n",
    "\n",
    "Sound changes tend to minimize articulatory effort while maintaining perceptual distinctiveness. This predicts that:\n",
    "\n",
    "1. Common sound changes involve small distances\n",
    "2. Sounds that rarely interchange have large distances\n",
    "3. Distance correlates with markedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat import calculate_distance\n",
    "\n",
    "# Common sound changes and their distances\n",
    "sound_changes = {\n",
    "    'Voicing': [('p', 'b'), ('t', 'd'), ('k', 'g'), ('f', 'v'), ('s', 'z')],\n",
    "    'Lenition': [('p', 'f'), ('t', 'θ'), ('k', 'x'), ('b', 'v'), ('g', 'ɣ')],\n",
    "    'Palatalization': [('k', 'tʃ'), ('g', 'dʒ'), ('t', 'tʃ'), ('d', 'dʒ')],\n",
    "    'Nasalization': [('b', 'm'), ('d', 'n'), ('g', 'ŋ')],\n",
    "    'Vowel raising': [('e', 'i'), ('o', 'u'), ('ɛ', 'e'), ('ɔ', 'o')]\n",
    "}\n",
    "\n",
    "change_distances = {}\n",
    "for change_type, pairs in sound_changes.items():\n",
    "    distances = []\n",
    "    for p1, p2 in pairs:\n",
    "        dist = calculate_distance(p1, p2, method='hamming')\n",
    "        if dist is not None:\n",
    "            distances.append(dist)\n",
    "    \n",
    "    if distances:\n",
    "        change_distances[change_type] = np.mean(distances)\n",
    "\n",
    "# Sort by average distance\n",
    "sorted_changes = sorted(change_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"Common Sound Changes Ranked by Distance:\")\n",
    "print(\"(Lower distance = more natural change)\\n\")\n",
    "for change_type, avg_dist in sorted_changes:\n",
    "    print(f\"{change_type:15} avg distance: {avg_dist:.3f}\")\n",
    "    examples = sound_changes[change_type][:3]\n",
    "    ex_str = ', '.join([f\"{p1}→{p2}\" for p1, p2 in examples])\n",
    "    print(f\"  Examples: {ex_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Framework\n",
    "\n",
    "### The Cognate Hypothesis\n",
    "\n",
    "Our fundamental hypothesis is:\n",
    "\n",
    "> **Words that are cognates (derived from a common ancestor) should have lower average phonetic distance than random word pairs**\n",
    "\n",
    "This can be formalized as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distfeat.alignment import align_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Simulated cognate data\n",
    "cognate_sets = [\n",
    "    # Cognate set 1: \"mother\"\n",
    "    [['m', 'ʌ', 'ð', 'ər'],      # English\n",
    "     ['m', 'u', 't', 'ər'],       # German\n",
    "     ['m', 'a', 't', 'ər']],      # Latin\n",
    "    \n",
    "    # Cognate set 2: \"night\"\n",
    "    [['n', 'aɪ', 't'],            # English\n",
    "     ['n', 'a', 'x', 't'],        # German\n",
    "     ['n', 'ɔ', 'k', 's']],       # Latin\n",
    "    \n",
    "    # Cognate set 3: \"heart\"  \n",
    "    [['h', 'ɑː', 't'],            # English\n",
    "     ['h', 'ɛ', 'r', 'ts'],       # German\n",
    "     ['k', 'ɔ', 'r']]             # Latin\n",
    "]\n",
    "\n",
    "# Calculate intra-cognate distances\n",
    "intra_distances = []\n",
    "for cognate_set in cognate_sets:\n",
    "    for i in range(len(cognate_set)):\n",
    "        for j in range(i + 1, len(cognate_set)):\n",
    "            result = align_sequences(cognate_set[i], cognate_set[j])\n",
    "            intra_distances.append(result.normalized_distance)\n",
    "\n",
    "# Calculate inter-cognate distances (between different sets)\n",
    "inter_distances = []\n",
    "for i in range(len(cognate_sets)):\n",
    "    for j in range(i + 1, len(cognate_sets)):\n",
    "        # Compare first word from each set\n",
    "        result = align_sequences(cognate_sets[i][0], cognate_sets[j][0])\n",
    "        inter_distances.append(result.normalized_distance)\n",
    "\n",
    "# Statistical test\n",
    "mean_intra = np.mean(intra_distances)\n",
    "mean_inter = np.mean(inter_distances)\n",
    "std_intra = np.std(intra_distances)\n",
    "std_inter = np.std(inter_distances)\n",
    "\n",
    "print(\"Cognate Distance Hypothesis Test:\")\n",
    "print(f\"\\nIntra-cognate distances:\")\n",
    "print(f\"  Mean: {mean_intra:.3f}\")\n",
    "print(f\"  Std:  {std_intra:.3f}\")\n",
    "print(f\"  N:    {len(intra_distances)}\")\n",
    "\n",
    "print(f\"\\nInter-cognate distances:\")\n",
    "print(f\"  Mean: {mean_inter:.3f}\")\n",
    "print(f\"  Std:  {std_inter:.3f}\")\n",
    "print(f\"  N:    {len(inter_distances)}\")\n",
    "\n",
    "print(f\"\\nDifference: {mean_inter - mean_intra:.3f}\")\n",
    "print(f\"Effect size (Cohen's d): {(mean_inter - mean_intra) / np.sqrt((std_intra**2 + std_inter**2) / 2):.2f}\")\n",
    "\n",
    "if mean_intra < mean_inter:\n",
    "    print(\"\\n✓ Hypothesis confirmed: Cognates have lower distances\")\n",
    "else:\n",
    "    print(\"\\n✗ Hypothesis rejected: Check data or method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The theoretical foundation of distfeat rests on:\n",
    "\n",
    "1. **Phonetic Features**: Binary representations based on articulatory and acoustic properties\n",
    "2. **Distance Metrics**: Multiple ways to quantify similarity, each with different properties\n",
    "3. **Information Theory**: Feature importance and entropy for optimization\n",
    "4. **Linguistic Validation**: Using cognate data to validate and optimize distances\n",
    "5. **Optimality Theory**: Predicting natural sound changes through distance minimization\n",
    "\n",
    "This foundation enables principled, reproducible measurement of phonetic similarity for computational historical linguistics.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Continue to [Chapter 3: Implementation Details](03_implementation.ipynb) to see how these concepts are implemented\n",
    "- Jump to [Tutorials](../tutorials/00_quickstart.ipynb) to start using distfeat\n",
    "- Explore [Case Studies](../case_studies/indo_european.ipynb) to see applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}